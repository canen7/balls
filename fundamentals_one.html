<!-- Create a program that goes through each value in array x, where x is [3,5,'Dojo', 'rocks', {name: 'Michael', title: 'Sensei'}]. Have it display each value of the array as well as the type of each value. For example, it should say or log "3: 'number', 5: 'number', 'rocks': 'string', ...".
Add a new value in the array x using a push method. New value you should add is 100.
Add a new array as the last member of the array then log x in the console and analyze how x looks.
Create a simple for loop that sums all the numbers between 1 to 500. Have console log the final sum.
Create another simple for loop that sums all the numbers between 1 to 1 million. Now, we want you to figure out how long in milliseconds this operation takes. Study http://www.w3schools.com/jsref/jsref_gettime.asp to learn more about how you can get the time in milliseconds. You can basically get the time of the operation by getting this timestamp before you start the for loop and getting another timestamp after the for loop is over. By subtracting the two timestamps, you can obtain how long this operation took. -->

<html>
<head>
	<title>Basic I</title>
	<script type="text/javascript">
	x = [3,5,'Dojo','rocks', {name:"michael", title:"Sensei"}];
	x.push(100);
	x.push([]);
	for (i in x)
		{
			console.log(x[i]);
		}
	var sum1=0;
	for (var i=0;i<500;i++) 
	{
		sum1 = sum1 + i;
	}
	console.log("the sum 1 is " + sum1);

	var d1 = new Date();
	var time1 = d1.getTime();
	
	var sum2=0;
	for (var i=0; i<=1000000; i++)
	{
		sum2 = sum2 + i;
	}
	
	var d2 =new Date();
	var time2 = d2.getTime();

	console.log ("the sum2 is "+ sum2)
	console.log(time2);
	console.log(time1);
	console.log(time2-time1);


	</script>
</head>
<body>

</body>
</html>